{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation Study Example Notebook\n",
    " \n",
    "This notebook provides an example workflow for generating comparative plots from RL training experiments in the CoRL framework. This includes training, evaluating and plotting policy performance data. This example will use the safe-autonomy-sims' 3D Satellite Inspection environment.\n",
    " \n",
    "Author: John McCarroll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Training\n",
    " \n",
    "We will first need to train a couple policy networks to evaluate. Below, we will walk through how to configure and run a single training job or trial, for evaluation. We will use the base Inspection environment (with PPO and no Runtime Assurance), but configure training to run with different seeds to evaluate the effect of random initialization on the learned policy."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1a - Setting Seeds\n",
    " \n",
    "Navigate to the Inspection environment's task config: 'safe-autonomy-sims/configs/tasks/cwh3d_inspection/cwh3d_task.yml'. Find the `env_config_updates` dict definition (line 24) and add a new item underneath named 'seed'. It should look something like this:\n",
    " \n",
    "``` \n",
    "env_config_updates: &env_config_updates\n",
    " TrialName: CWH-3D-INSPECTION\n",
    " output_path: /tmp/safe_autonomy/\n",
    " seed: 11\n",
    "```\n",
    " \n",
    "For this example, I will be comparing the seeds 11 and 843.\n",
    " \n",
    " \n",
    " \n",
    "### Step 1b - Configuring Checkpoints\n",
    " \n",
    "Evaluation data will be generated AFTER training. This means it is crucial to __save out policy checkpoints during training__. Using the same task config file from before, let's navigate to the `tune_config_updates` dict definition (~line 29). We are primarily concerned with the `checkpoint_freq` (the number of iterations between saved checkpoints) and `keep_checkpoints_num` (the maximum number of checkpoints to save) variables. Here's an example configuration:\n",
    " \n",
    "```\n",
    "tune_config_updates: &tune_config_updates\n",
    " local_dir: /tmp/safe_autonomy/ray_results/\n",
    " checkpoint_freq: 5\n",
    " keep_checkpoints_num: 400\n",
    " stop:\n",
    "   training_iteration: 50\n",
    "```\n",
    " \n",
    "Notes:\n",
    "   - The 'stop' subdictionary defines training termination criteria. Above, training is configured to terminate after 50 iterations.\n",
    "   - The string given to the 'local_dir' key is the path at which our training output (and checkpoints) will be saved. Tune will create a new directory with a long name consisting of the TrialName (defined in the environment config), hyperparameter values, and a timestamp. This directory will hold the checkpoints needed for evaluation. We will need to aggregate our experiment output directory paths into a single dictionary in a later step.\n",
    " \n",
    " \n",
    " \n",
    "### Step 1c - Running Training\n",
    " \n",
    "All that is left in this portion is to run CoRL's train_rl.py script. For that we need an experiment config. The Inspection environment's experiment config is located at 'safe-autonomy-sims/configs/experiments/inspection/inspection_3d.yml'. Ensure that this experiment config contains the path to the task config file we modified in the previous step.\n",
    " \n",
    "To launch a training job, open a terminal and navigate to the root of your local safe-autonomy-sims repository. Then, enter the command found below:\n",
    " \n",
    "```shell\n",
    "$ python -m corl.train_rl --cfg /path/to/inspection/experiment/config\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Set Up for Evaluation\n",
    " \n",
    "Now that we have trained a couple of policies, let's set up to run CoRL's Evaluation Framework in order to generate data for our comparative plot. We will need a PlatformSerializer class, the experiment and task config file paths, a dictionary of `experiment names` to `experiment_state` file paths KVPs, a plot output path, and a plot config dictionary."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2a - PlatformSerializer\n",
    " \n",
    "The Evaluation Framework makes use of a PlatformSerializer class to save and load platform state. These PlatformSerializer classes are specific to the Platforms used in our experiment. We can create a PlatformSerializer for a custom Platform by extending the `corl.evaluation.runners.section_factories.plugins.platform_serializer.PlatformSerializer` class. Thankfully, we have a PlatformSerializer class prepared for our Inspection environment. We'll import it below, along with the `run_ablation_study` function we will use from the Evaluation API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from saferl.evaluation.evaluation_api import run_ablation_study, create_sample_complexity_plot\n",
    "\n",
    "# import our serializer for CWH platforms\n",
    "from saferl.evaluation.launch.serialize_cwh3d import SerializeCWH3D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2b - Defining Variables\n",
    "\n",
    "Now that we have trained a few policies, it's time to aggregate and evaluate that data. Let's begin defining some arguments needed to run our ablation study. We will need to define the paths for our training job's experiment config and task config. We will also define the path where we want our comparative (sample complexity) plot to be saved. Aside from simple paths, there are four key arguments to define, which determine much of the function of the evaluation framework. Below, we discuss the details of each argument.\n",
    "\n",
    "\n",
    "#### Training Output Directories\n",
    "Create a dictionary of \"experiment name\" to \"experiment_state file path\" KVPs. The experiment_state file names will follow the convention of `experiment_state-[datatime].pkl`. The keys will be used as labels on the generated plot, so the user is encouraged to give them semantic meaning to their experiment. See the `training_outputs` variable below for an example. These experiment_state paths will be used to find checkpoint subdirectories. policies will be loaded from any found checkpoints and used to run evaluation episodes.\n",
    "\n",
    "\n",
    "#### Metrics Config\n",
    "This dict defines the scope, name, class path, and config of every Metric to be calculated during evaluation episodes. The metrics_config has two 'scopes': 'world' and 'agent'. Metrics defined in the 'world' scope apply to the generic Episode data, whereas metrics defined in the 'agent' scope apply specifically to data available to agents. The 'agent' key is assigned a dictionary of lists. The keys are either a specific agent's name or '\\__default__' (if we want the Metric applied to all agents). The lists contain dictionary definitions of each Metric, which must include a 'name', 'functor', and 'config'. See the cell below for a simple example `metric_config`, defining only the TotalReward Metric in the default agent scope.\n",
    "The nested structure of the `metrics_config` is based off of the yaml config file used in CoRL's evaluation framework. If it is difficult to follow, the usage guide, found [here](https://github.com/act3-ace/act3-rl/corl/-/blob/main/docs/evaluation_framework/user_guide.md#521-write-the-metrics-configuration-file), contains more details on Metrics configuration.\n",
    "\n",
    "\n",
    "#### TestCaseManager Config\n",
    "The TestCaseManager is a class behind the scenes of the evaluation framework responsible for coordinating the parameter providers and the initial parameter values used in each evaluation episode or \"test case\". This config is not required. If a `test_case_manager_config` kwarg is not defined for the `run_ablation_study` or `evaluate methods`, then the task's default ParameterProviders will be used to initialize the environment state for each evaluation episode. If a specific TestCaseStrategy is desired to define an explicit set of test cases for each policy evaluation, then one only needs to define two items in the `test_case_manager_config` dict. The first, `class_path`, is the class path to the TestCaseStrategy desired for the evaluation. The second item is `config`, which simply defines the kwargs passed to the TestCaseStrategy's constructor.\n",
    "\n",
    "Below, the default `test_case_manager_config` is redundantly defined, for example purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables\n",
    "\n",
    "training_outputs = {\n",
    "    'experiment_name1': '/absolute/path/to/experiment_state/file.pkl',\n",
    "    'experiment_name2': '/absolute/path/to/experiment_state/file.pkl'\n",
    "}\n",
    "expr_config = '/absolute/path/to/experiment/config/file.yml'\n",
    "task_config_path = '/absolute/path/to/task/config/file.yml'\n",
    "\n",
    "metrics_config = {\n",
    "    \"agent\": {\n",
    "        \"__default__\": [\n",
    "            {\n",
    "                \"name\": \"TotalReward\",\n",
    "                \"functor\": \"corl.evaluation.metrics.generators.rewards.TotalReward\",\n",
    "                \"config\": {\n",
    "                    \"description\": \"total reward calculated from test case rollout\"\n",
    "                }\n",
    "            },\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "default_test_case_manager_config = {\n",
    "    \"class_path\": \"corl.evaluation.runners.section_factories.test_cases.default_strategy.DefaultStrategy\",\n",
    "    \"config\": {\n",
    "        \"num_test_cases\": 3\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Generate Evaluation Data\n",
    "\n",
    "Now that we've defined the required arguments for running an ablation study in CoRL's Evaluation Framework, it's time to generate some data. We'll use the `run_ablation_study` function, which sequentially loads each checkpoint policy, executes the defined evaluation episodes with said policy, calculates Metrics, organizes the data into a `pandas.DataFrame`, and optionally saves a seaborn plot to the specified location. The `run_ablation_study` function returns a `pandas.DataFrame` populated with Metric data calculated from the Evaluation Episodes. We'll use this DataFrame in the next step.\n",
    "Note: the `run_ablation_study` function can take a long time to execute depending on the speed of the provided environment's Simulator and the number of checkpoint policies that need to be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch ablation study pipeline\n",
    "dataframe = run_ablation_study(\n",
    "    training_outputs,\n",
    "    task_config_path,\n",
    "    expr_config,\n",
    "    metrics_config,\n",
    "    SerializeCWH3D,\n",
    "    test_case_manager_config=default_test_case_manager_config\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Visualize Evaluation Data\n",
    "\n",
    "Now that we have Evaluation Metrics data organized in a `pandas.DataFrame`, let's create a visualization. We will use the `create_sample_complexity` function to generate a plot comparing the average Reward each checkpoint's policy achieved during Evaluation Episodes. This will allow us to compare the effect of seeding our Inspection Environment with respect to the performance of the resulting trained policies. We need to pass the `data`, `plot_output` path, and `plot_config` as arguments to our `create_sample_complexity` plot. The keys in the `plot_config` are named after keyword arguments used to configure the data displayed in the plot. Find an explanation of some important arguments below.\n",
    "\n",
    "#### Plot Config\n",
    "This dictionary defines the kwargs passed to seaborn's lineplot method to create the sample complexity plot. Only two kwargs are required: `xaxis` and `yaxis`. The `xaxis` variable has three supported units of training time: `num_interactions`, `num_episodes`, and `iterations`. The `yaxis` variable is a bit more flexible, as it can be assigned to the name of any Metric found in the metrics_config (as long as the Metric has a numeric value). This allows users to create comparative training plots of custom Metrics values (ex. safety violations or runtime assurance interventions). For this example, we will use the TotalReward metric.\n",
    "\n",
    "Other kwargs can include anything accepted by seaborn's lineplot method. Take `hue`, for example, which groups experiment data together based on categorical data (such as the `experiment` name column). Another kwarg worth mentioning is `ci`, which stands for confidence interval. This kwarg sets the statistical error bound visualized on the resulting plot. By default, `ci` is assigned the value 95 (as in a confidence interval of 95 percent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample complexity plot\n",
    "plot_ouput = '/absolute/path/to/desired/plot/save/location.png'\n",
    "\n",
    "plot_config = {\n",
    "    \"yaxis\": \"TotalReward\",\n",
    "    \"xaxis\": \"timesteps_total\"\n",
    "}\n",
    "\n",
    "axis = create_sample_complexity_plot(dataframe, plot_ouput, **plot_config)\n",
    "\n",
    "# Note: the two asterisks in front of the plot_config convert the items in the plot_config dict into keyword arguments\n",
    "#       ex. my_function( **{'x_axis': 'episodes'} ) -> my_function(x_axis='episodes')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safe2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95f5df37becb13747f9bba1c971eece80afb0a1e2b0eb3d40d04d43fa91d52b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
